{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate seqeval \"transformers>=4.30.0\""
   ],
   "metadata": {
    "id": "fTzK4ulCxHqu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqMfEWE_u1Ax"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def read_cupt(path):\n",
    "    \"\"\"\n",
    "    Returns a list of sentences.\n",
    "    Each sentence is a list of token dicts:\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"form\": \"ran\",\n",
    "        \"lemma\": \"_\",\n",
    "        ...\n",
    "        \"mwe\": \"1:VID\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                if current:\n",
    "                    sentences.append(current)\n",
    "                    current = []\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            cols = line.split(\"\\t\")\n",
    "            if \"-\" in cols[0]:  # multi-word token line\n",
    "                continue\n",
    "\n",
    "            tok = {\n",
    "                \"id\": cols[0],\n",
    "                \"form\": cols[1],\n",
    "                \"lemma\": cols[2],\n",
    "                \"upos\": cols[3],\n",
    "                \"xpos\": cols[4],\n",
    "                \"feats\": cols[5],\n",
    "                \"head\": cols[6],\n",
    "                \"deprel\": cols[7],\n",
    "                \"deps\": cols[8],\n",
    "                \"misc\": cols[9],\n",
    "                \"mwe\": cols[10]\n",
    "            }\n",
    "            current.append(tok)\n",
    "\n",
    "    if current:\n",
    "        sentences.append(current)\n",
    "\n",
    "    return sentences"
   ],
   "metadata": {
    "id": "UtaCee_ru7ut"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def parse_mwe_column(col):\n",
    "    \"\"\"\n",
    "    Handle:\n",
    "        \"_\", \"*\"           → empty\n",
    "        \"1\"                → continuation of MWE 1 (no type)\n",
    "        \"1:VID\"            → start of MWE 1 type VID\n",
    "        \"1;2\"              → continuation of both\n",
    "        \"1;2:VID\"          → mix\n",
    "    \"\"\"\n",
    "    if col in (\"_\", \"*\", \"\"):\n",
    "        return []\n",
    "\n",
    "    entries = []\n",
    "    for part in col.split(\";\"):\n",
    "        if \":\" in part:\n",
    "            num, typ = part.split(\":\")\n",
    "            entries.append((int(num), typ))\n",
    "        else:\n",
    "            entries.append((int(part), None))\n",
    "    return entries\n",
    "\n",
    "\n",
    "############################################\n",
    "# 3. Convert CUPT → BIO\n",
    "############################################\n",
    "\n",
    "def normalize_label(lbl):\n",
    "    \"\"\"\n",
    "    Enforce single label:\n",
    "    \"B-LVC.full;B-VID\" → \"B-LVC.full\"\n",
    "    \"\"\"\n",
    "    return lbl.split(\";\")[0]\n",
    "\n",
    "\n",
    "def cupt_to_bio(sent):\n",
    "    \"\"\"\n",
    "    Converts one sentence (list of tokens) → list of BIO labels.\n",
    "    Handles multi-MWE, continuations, etc.\n",
    "    Always returns ONE label per token (normalized).\n",
    "    \"\"\"\n",
    "\n",
    "    # collect known types (first non-None determines type)\n",
    "    mwe_types = {}\n",
    "    for tok in sent:\n",
    "        for mwe_id, typ in parse_mwe_column(tok[\"mwe\"]):\n",
    "            if typ is not None:\n",
    "                mwe_types[mwe_id] = typ\n",
    "\n",
    "    tags = []\n",
    "    started = {}\n",
    "\n",
    "    for tok in sent:\n",
    "        entries = parse_mwe_column(tok[\"mwe\"])\n",
    "\n",
    "        if not entries:\n",
    "            tags.append(\"O\")\n",
    "            continue\n",
    "\n",
    "        token_labels = []\n",
    "        for mwe_id, typ in entries:\n",
    "            the_type = mwe_types.get(mwe_id, \"UNK\")\n",
    "\n",
    "            if mwe_id not in started:\n",
    "                token_labels.append(f\"B-{the_type}\")\n",
    "                started[mwe_id] = True\n",
    "            else:\n",
    "                token_labels.append(f\"I-{the_type}\")\n",
    "\n",
    "        # enforce single selection\n",
    "        final = normalize_label(\";\".join(token_labels))\n",
    "        tags.append(final)\n",
    "\n",
    "    return tags\n",
    "\n",
    "def extract_label_set(train_sents):\n",
    "    labels = set()\n",
    "    for sent in train_sents:\n",
    "        bio = cupt_to_bio(sent)\n",
    "        for t in bio:\n",
    "            labels.add(normalize_label(t))\n",
    "    return sorted(labels)\n",
    "\n",
    "def extract_label_set_from(*sentence_lists):\n",
    "    \"\"\"\n",
    "    Build the sorted set of labels (BIO single-label normalized) found\n",
    "    across all provided sentence lists.\n",
    "\n",
    "    Each argument is a list of sentences (as returned by read_cupt).\n",
    "    \"\"\"\n",
    "    labels = set()\n",
    "    for sents in sentence_lists:\n",
    "        for sent in sents:\n",
    "            bio = cupt_to_bio(sent)\n",
    "            for t in bio:\n",
    "                labels.add(normalize_label(t))\n",
    "    return sorted(labels)"
   ],
   "metadata": {
    "id": "gRfUPLpKu_4M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class ParsemeDataset:\n",
    "    def __init__(self, sentences, tokenizer, label2id, max_length=256):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.id2label = {v: k for k, v in label2id.items()}\n",
    "        self.max_length = max_length\n",
    "        self._unknown_label_warned = False  # single-time warning\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.sentences[idx]\n",
    "        words = [tok[\"form\"] for tok in sent]\n",
    "        labels = [normalize_label(t) for t in cupt_to_bio(sent)]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "\n",
    "        # align BIO labels to wordpieces\n",
    "        word_ids = encoding.word_ids()\n",
    "        label_ids = []\n",
    "        prev_word = None\n",
    "\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                if wid != prev_word:\n",
    "                    lab = labels[wid]\n",
    "                    if lab not in self.label2id:\n",
    "                        # fallback: map unseen label to \"O\"\n",
    "                        if not self._unknown_label_warned:\n",
    "                            print(f\"Warning: unseen label '{lab}' encountered — mapping to 'O'.\")\n",
    "                            self._unknown_label_warned = True\n",
    "                        lab = \"O\"\n",
    "                    label_ids.append(self.label2id[lab])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            prev_word = wid\n",
    "\n",
    "        encoding.pop(\"offset_mapping\")\n",
    "        encoding[\"labels\"] = label_ids\n",
    "        return encoding"
   ],
   "metadata": {
    "id": "vGCdmMFJvDgN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def clean_pred_label(lbl):\n",
    "    return lbl.split(\";\")[0]\n",
    "\n",
    "\n",
    "def align_predictions(predictions, label_ids, id2label):\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    batch_preds = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for pred_seq, gold_seq in zip(preds, label_ids):\n",
    "        p_list = []\n",
    "        l_list = []\n",
    "\n",
    "        for p, g in zip(pred_seq, gold_seq):\n",
    "            if g == -100:\n",
    "                continue\n",
    "            p_lbl = clean_pred_label(id2label[int(p)])\n",
    "            g_lbl = clean_pred_label(id2label[int(g)])\n",
    "            p_list.append(p_lbl)\n",
    "            l_list.append(g_lbl)\n",
    "\n",
    "        batch_preds.append(p_list)\n",
    "        batch_labels.append(l_list)\n",
    "\n",
    "    return batch_preds, batch_labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds_list, labels_list = align_predictions(\n",
    "        logits, labels, model.config.id2label\n",
    "    )\n",
    "\n",
    "    results = seqeval.compute(\n",
    "        predictions=preds_list,\n",
    "        references=labels_list\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results.get(\"overall_accuracy\", 0.0)\n",
    "    }"
   ],
   "metadata": {
    "id": "HD3zEnLbvJ9x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(train_file, dev_file, output_dir, model_name=\"xlm-roberta-base\"):\n",
    "    print(\"Reading data...\")\n",
    "    train_sents = read_cupt(train_file)\n",
    "    dev_sents = read_cupt(dev_file)\n",
    "\n",
    "    print(\"Extracting labels from train+dev (to avoid unseen labels)...\")\n",
    "    labels = extract_label_set_from(train_sents, dev_sents)\n",
    "    if \"O\" not in labels:\n",
    "        labels = [\"O\"] + labels  # ensure 'O' present\n",
    "    label2id = {l: i for i, l in enumerate(labels)}\n",
    "    id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "    print(\"Loading model & tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    global model\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_dataset = ParsemeDataset(train_sents, tokenizer, label2id)\n",
    "    dev_dataset = ParsemeDataset(dev_sents, tokenizer, label2id)\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    print(\"Done!\")\n"
   ],
   "metadata": {
    "id": "zaJ0VKpbzDgl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def bio_to_parseme_tags(bio_tags):\n",
    "    \"\"\"\n",
    "    Convert BIO tags (e.g. B-LVC.full, I-LVC.full, O)\n",
    "    into official PARSEME MWE tags:\n",
    "      - O            → \"*\"\n",
    "      - B-TYPE       → \"1:TYPE\"\n",
    "      - I-TYPE       → \"1\"\n",
    "    Supports multiple MWEs via incremental numbering.\n",
    "    \"\"\"\n",
    "\n",
    "    mwe_id_counter = 1\n",
    "    active_mwes = {}  # mwe_type -> assigned ID\n",
    "    result = []\n",
    "\n",
    "    for tag in bio_tags:\n",
    "        if tag == \"O\":\n",
    "            result.append(\"*\")\n",
    "            continue\n",
    "\n",
    "        bio, mwe_type = tag.split(\"-\", 1)\n",
    "\n",
    "        if bio == \"B\":\n",
    "            # Start new MWE\n",
    "            active_mwes[mwe_type] = mwe_id_counter\n",
    "            result.append(f\"{mwe_id_counter}:{mwe_type}\")\n",
    "            mwe_id_counter += 1\n",
    "\n",
    "        elif bio == \"I\":\n",
    "            # Continue existing MWE\n",
    "            if mwe_type in active_mwes:\n",
    "                result.append(str(active_mwes[mwe_type]))\n",
    "            else:\n",
    "                # Inconsistent segmentation: treat as new MWE\n",
    "                active_mwes[mwe_type] = mwe_id_counter\n",
    "                result.append(f\"{mwe_id_counter}:{mwe_type}\")\n",
    "                mwe_id_counter += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fill_cupt_with_predictions(\n",
    "    model_dir: str,\n",
    "    input_cupt_path: str,\n",
    "    output_cupt_path: str,\n",
    "    id2label: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a trained Parseme MWE model and fills the last column of a .cupt file\n",
    "    with predicted PARSEME-compliant MWE labels (using \"*\" for O tags).\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # ---- Utilities ---------------------------------------------------------\n",
    "\n",
    "    def read_cupt(path):\n",
    "        sents = []\n",
    "        cur = []\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip() == \"\":\n",
    "                    if cur:\n",
    "                        sents.append(cur)\n",
    "                        cur = []\n",
    "                elif line.startswith(\"#\"):\n",
    "                    cur.append(line)\n",
    "                else:\n",
    "                    fields = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                    cur.append(fields)\n",
    "        if cur:\n",
    "            sents.append(cur)\n",
    "        return sents\n",
    "\n",
    "    def write_cupt(sents, path):\n",
    "        with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "            for sent in sents:\n",
    "                for line in sent:\n",
    "                    if isinstance(line, str):\n",
    "                        f.write(line)\n",
    "                    else:\n",
    "                        f.write(\"\\t\".join(line) + \"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    sents = read_cupt(input_cupt_path)\n",
    "    print(f\"Loaded {len(sents)} sentences.\")\n",
    "\n",
    "    for sent in sents:\n",
    "\n",
    "        tokens = [fields[1] for fields in sent if isinstance(fields, list)]\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,  # used only for alignment\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].to(device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            ).logits\n",
    "\n",
    "        pred_ids = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "        word_ids = encoded.word_ids()\n",
    "\n",
    "        # convert subword predictions → per-token predictions\n",
    "        token_preds = []\n",
    "        last_word = None\n",
    "        for pred_id, word_idx in zip(pred_ids, word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            if word_idx != last_word:\n",
    "                token_preds.append(id2label[pred_id])\n",
    "                last_word = word_idx\n",
    "\n",
    "        assert len(token_preds) == len(tokens), \"Alignment mismatch!\"\n",
    "\n",
    "        # ---- Convert to PARSEME format ------------------------------------\n",
    "        parseme_tags = bio_to_parseme_tags(token_preds)\n",
    "\n",
    "        # ---- Write into last column ---------------------------------------\n",
    "        idx = 0\n",
    "        for row in sent:\n",
    "            if isinstance(row, list):\n",
    "                row[-1] = parseme_tags[idx]\n",
    "                idx += 1\n",
    "\n",
    "    write_cupt(sents, output_cupt_path)\n",
    "    print(f\"Wrote predictions to: {output_cupt_path}\")"
   ],
   "metadata": {
    "id": "Q90VLuhszLO2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir = \"drive/MyDrive/datasets/parseme/subtask1/PL\""
   ],
   "metadata": {
    "id": "GOMm24ROvlJX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_model(\n",
    "    train_file=os.path.join(data_dir, \"train.cupt\"),\n",
    "    dev_file=os.path.join(data_dir, \"dev.cupt\"),\n",
    "    output_dir=\"pl_model\",\n",
    "    model_name=\"xlm-roberta-base\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "fKKRwAvsvjDf",
    "outputId": "9433980b-268d-4a44-fd37-d2de3a817af0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading data...\n",
      "Extracting labels from train+dev (to avoid unseen labels)...\n",
      "Loading model & tokenizer...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-2340408222.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preparing datasets...\n",
      "Training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14145' max='14145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14145/14145 23:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.096509</td>\n",
       "      <td>0.786541</td>\n",
       "      <td>0.735453</td>\n",
       "      <td>0.760140</td>\n",
       "      <td>0.975725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.087538</td>\n",
       "      <td>0.745361</td>\n",
       "      <td>0.788227</td>\n",
       "      <td>0.766195</td>\n",
       "      <td>0.975580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.087220</td>\n",
       "      <td>0.799866</td>\n",
       "      <td>0.805819</td>\n",
       "      <td>0.802831</td>\n",
       "      <td>0.980233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.109395</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.829499</td>\n",
       "      <td>0.827260</td>\n",
       "      <td>0.981318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.121054</td>\n",
       "      <td>0.849687</td>\n",
       "      <td>0.826116</td>\n",
       "      <td>0.837736</td>\n",
       "      <td>0.982812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "output_dir = \"pl_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(output_dir)"
   ],
   "metadata": {
    "id": "XXwbPICv3Dnf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_dir = \"./pl_model\"\n",
    "id2label = model.config.id2label\n",
    "\n",
    "fill_cupt_with_predictions(\n",
    "    model_dir,\n",
    "    \"pl.test.blind.cupt\",\n",
    "    \"pl_prediction.cupt\",\n",
    "    id2label\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fkEFDFh2XCJ",
    "outputId": "0e7a2649-484f-44d4-d8b8-23cee1b0223b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 1127 sentences.\n",
      "Wrote predictions to: pl_prediction.cupt\n"
     ]
    }
   ]
  }
 ]
}
